{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare the dataset\n",
    "\n",
    "You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the images to [-1, 1]\n",
    "train_images = (train_images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    train_images).shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    # Note: None is the batch size\n",
    "    assert model.output_shape == (None, 7, 7, 256)\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            128, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            1, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False, activation=\"tanh\"\n",
    "        )\n",
    "    )\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18f3630eb80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discriminator\n",
    "\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            64, (5, 5), strides=(2, 2), padding=\"same\", input_shape=[28, 28, 1]\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.0001621]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator loss\n",
    "\n",
    "\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminators decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator and the generator optimizers are different since you will train two networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoints\n",
    "\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./netural_network/training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss, discriminator.trainable_variables\n",
    "    )\n",
    "\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator, generator.trainable_variables)\n",
    "    )\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.savefig(\"image_at_epoch_{:04d}.png\".format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        print(\"Time for epoch {} is {} sec\".format(epoch + 1, time.time() - start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Citrusbug Technolabs\\AppData\\Local\\Temp\\ipykernel_15192\\3133860512.py\", line 10, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"c:\\Citrusbug-Project\\AI-ML_Learning\\venv_AI\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Citrusbug-Project\\AI-ML_Learning\\venv_AI\\lib\\site-packages\\keras\\src\\layers\\input_spec.py\", line 227, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling Sequential.call().\n    \n    \u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 6272, but received input with shape (28, 896)\u001b[0m\n    \n    Arguments received by Sequential.call():\n      • inputs=tf.Tensor(shape=(28, 28, 1), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Produce images for the GIF as you go\u001b[39;00m\n\u001b[0;32m      9\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Citrusbug-Project\\AI-ML_Learning\\venv_AI\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\CITRUS~1\\AppData\\Local\\Temp\\__autograph_generated_filenx_qnhud.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m gen_tape, ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m disc_tape:\n\u001b[0;32m     10\u001b[0m     generated_images \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generator), (ag__\u001b[38;5;241m.\u001b[39mld(noise),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m---> 11\u001b[0m     real_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), (ag__\u001b[38;5;241m.\u001b[39mld(images),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     12\u001b[0m     fake_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), (ag__\u001b[38;5;241m.\u001b[39mld(generated_images),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     13\u001b[0m     gen_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generator_loss), (ag__\u001b[38;5;241m.\u001b[39mld(fake_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mc:\\Citrusbug-Project\\AI-ML_Learning\\venv_AI\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Citrusbug-Project\\AI-ML_Learning\\venv_AI\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Citrusbug Technolabs\\AppData\\Local\\Temp\\ipykernel_15192\\3133860512.py\", line 10, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"c:\\Citrusbug-Project\\AI-ML_Learning\\venv_AI\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Citrusbug-Project\\AI-ML_Learning\\venv_AI\\lib\\site-packages\\keras\\src\\layers\\input_spec.py\", line 227, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling Sequential.call().\n    \n    \u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 6272, but received input with shape (28, 896)\u001b[0m\n    \n    Arguments received by Sequential.call():\n      • inputs=tf.Tensor(shape=(28, 28, 1), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
